from simple_stories_train.models.gpt2 import GPT2Config
from simple_stories_train.models.gpt2_simple import GPT2SimpleConfig
from simple_stories_train.models.llama import LlamaConfig
from simple_stories_train.models.llama_simple import LlamaSimpleConfig
from simple_stories_train.models.llama_simple_mlp import LlamaSimpleMlpConfig

MODEL_CONFIGS = {
    # Llama debug/dev sizes
    "llama-d2": LlamaConfig(
        block_size=1024,
        vocab_size=50257,
        n_layer=2,
        n_head=2,
        n_embd=12,
        rotary_dim=12 // 2,
        n_key_value_heads=2 // 2,
        flash_attention=True,
    ),
    "llama-d12": LlamaConfig(
        block_size=1024,
        vocab_size=50257,
        n_layer=12,
        n_head=12,
        n_embd=768,
        rotary_dim=768 // 12,
        n_key_value_heads=12 // 4,
        flash_attention=True,
    ),
    "llama-d24": LlamaConfig(
        block_size=1024,
        vocab_size=50257,
        n_layer=24,
        n_head=16,
        n_embd=1024,
        rotary_dim=1024 // 16,
        n_key_value_heads=16 // 4,
        flash_attention=True,
    ),
    "llama-d36": LlamaConfig(
        block_size=1024,
        vocab_size=50257,
        n_layer=36,
        n_head=20,
        n_embd=1280,
        rotary_dim=1280 // 20,
        n_key_value_heads=20 // 4,
        flash_attention=True,
    ),
    "llama-d48": LlamaConfig(
        block_size=1024,
        vocab_size=50257,
        n_layer=48,
        n_head=25,
        n_embd=1600,
        rotary_dim=1600 // 25,
        n_key_value_heads=25 // 4,
        flash_attention=True,
    ),
    # SimpleStories Llama presets
    "llama-1.25M": LlamaConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=4,
        n_head=4,
        n_embd=128,
        n_intermediate=128 * 4 * 2 // 3,
        rotary_dim=128 // 4,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=True,
    ),
    "llama-5M": LlamaConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=6,
        n_head=4,
        n_embd=256,
        n_intermediate=256 * 4 * 2 // 3,
        rotary_dim=256 // 6,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=True,
    ),
    "llama-11M": LlamaConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=6,
        n_head=6,
        n_embd=384,
        n_intermediate=384 * 4 * 2 // 3,
        rotary_dim=384 // 8,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=True,
    ),
    "llama-30M": LlamaConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=10,
        n_head=8,
        n_embd=512,
        n_intermediate=512 * 4 * 2 // 3,
        rotary_dim=512 // 8,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=True,
    ),
    "llama-35M": LlamaConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=12,
        n_head=8,
        n_embd=512,
        n_intermediate=512 * 4 * 2 // 3,
        rotary_dim=512 // 8,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=True,
    ),
    # GPT-2 presets
    "gpt2-1.25M": GPT2Config(
        block_size=512,
        vocab_size=4019,
        n_layer=4,
        n_head=4,
        n_embd=128,
        flash_attention=True,
    ),
    "gpt2_simple-1.25M": GPT2SimpleConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=4,
        n_head=4,
        n_embd=128,
        flash_attention=False,
    ),
    "gpt2_simple-1L": GPT2SimpleConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=1,
        n_head=4,
        n_embd=128,
        flash_attention=False,
    ),
    "gpt2_simple-2L": GPT2SimpleConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=2,
        n_head=4,
        n_embd=128,
        flash_attention=False,
    ),
    "llama_simple-1.25M": LlamaSimpleConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=4,
        n_head=4,
        n_embd=128,
        n_intermediate=128 * 4 * 2 // 3,
        rotary_dim=128 // 4,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=False,
    ),
    "llama_simple-1L": LlamaSimpleConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=1,
        n_head=4,
        n_embd=128,
        n_intermediate=128 * 4 * 2 // 3,
        rotary_dim=128 // 4,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=False,
    ),
    "llama_simple-2L": LlamaSimpleConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=2,
        n_head=4,
        n_embd=128,
        n_intermediate=128 * 4 * 2 // 3,
        rotary_dim=128 // 4,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=False,
    ),
    # LlamaSimpleMlp presets (GELU MLP instead of SwiGLU)
    "llama_simple_mlp-1.25M": LlamaSimpleMlpConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=4,
        n_head=4,
        n_embd=128,
        n_intermediate=128 * 4,
        rotary_dim=128 // 4,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=False,
    ),
    "llama_simple_mlp-1L": LlamaSimpleMlpConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=1,
        n_head=4,
        n_embd=128,
        n_intermediate=128 * 4,
        rotary_dim=128 // 4,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=False,
    ),
    "llama_simple_mlp-2L": LlamaSimpleMlpConfig(
        block_size=512,
        vocab_size=4019,
        n_layer=2,
        n_head=4,
        n_embd=128,
        n_intermediate=128 * 4,
        rotary_dim=128 // 4,
        n_ctx=512,
        n_key_value_heads=2,
        flash_attention=False,
    ),
}
