wandb_project: spd
train_dataset_config:
  name: monology/pile-uncopyrighted
  is_tokenized: false
  tokenizer_file_path: null
  hf_tokenizer_path: gpt2
  split: train
  streaming: true  # Required for Pile (very large dataset)
  n_ctx: 512
  seed: 0
  column_name: text
val_dataset_config:
  # Use NeelNanda/pile-10k for validation (smaller, faster)
  name: NeelNanda/pile-10k
  is_tokenized: false
  tokenizer_file_path: null
  hf_tokenizer_path: gpt2
  split: train
  streaming: false
  n_ctx: 512
  seed: 0
  column_name: text

model_id: gpt2_simple-pile-4L
dtype: float32  # Without this, non-flash attention will cause divergence
batch_size: 16  # 8 GPUs
total_batch_size: 65536 # 16 * 512 * ddp_world_size (8)
num_iterations: 500_000
warmup_iters: 600

learning_rate: 3e-4
learning_rate_decay_frac: 0.1
weight_decay: 0.1
grad_clip: 1.0
val_loss_every: 100
val_max_steps: 20
sample_every: 1000
intermediate_checkpoints: true

compile: true
flash_attention: false
tensorcores: true
