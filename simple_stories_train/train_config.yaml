wandb_project: spd
train_dataset_config:
  name: SimpleStories/SimpleStories
  is_tokenized: false
  tokenizer_file_path: simple_stories_train/tokenizer/simplestories-tokenizer.json
  split: train
  streaming: false
  n_ctx: 512
  seed: 0
  column_name: story
val_dataset_config:
  name: SimpleStories/SimpleStories
  is_tokenized: false
  tokenizer_file_path: simple_stories_train/tokenizer/simplestories-tokenizer.json
  split: test
  streaming: false
  n_ctx: 512
  seed: 0
  column_name: story
# model_id: llama-1.25M
model_id: gpt2_simple-1.25M
# 1 GPU
batch_size: 64
total_batch_size: 32768 # 64 * 512
num_iterations: 100_000 # custom iterations, reload the train loader if we exhaust data
warmup_iters: 600
# # 4 GPUs
# batch_size: 64
# total_batch_size: 262144 # 64 * 1024 * 4
# num_iterations: 2410 # (617806 dataset rows / 64 batch size / 4)
# warmup_iters: 50
learning_rate: 1e-4
learning_rate_decay_frac: 0.1
weight_decay: 0.1
grad_clip: 1.0
val_loss_every: 100
val_max_steps: 20
sample_every: 1000
intermediate_checkpoints: false

# For finetuning (and ablating LN stds)
# from_pretrained: "wandb:goodfire/spd/runs/syhzse3u"
# num_iterations: 9000
# enable_ln_ablation: true
# ln_stats_path: out/syhzse3u-ln-stats.yaml
# n_steps_between_ln_ablation: 500
# ln_ablation_start_step: 1
# ln_ablation_max_to_remove: null
